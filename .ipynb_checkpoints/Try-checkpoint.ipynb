{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ca9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcc93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyopengl\n",
    "#!pip install pyglet==1.5.27\n",
    "#!pip install stable_baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cb4d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv #multiple agents at a time, Wraping in dummy enviroment i think\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #Evaluate success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fce206c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cafe3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8f6ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: -200.0\n",
      "Episode: 2 Score: -200.0\n",
      "Episode: 3 Score: -200.0\n",
      "Episode: 4 Score: -200.0\n",
      "Episode: 5 Score: -200.0\n"
     ]
    }
   ],
   "source": [
    "episode = 5\n",
    "for episode in range(1,episode+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {}'.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb91ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae52197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebf8e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53686a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621bc22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32944986, 0.0634734 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66e290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe987cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a93a652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bcc02fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda : env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d2107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO?? #To understand parameters of PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3501d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_6\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 495  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003848855 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 3.8e-05     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000904   |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 327         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010677362 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.0149     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 87.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002529479 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.00732    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000173   |\n",
      "|    value_loss           | 83.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 315          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0115239965 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.0108      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 68.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066579147 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.000117    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.85         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    value_loss           | 53.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 305          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009724825 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.00125     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 5.99e-05     |\n",
      "|    value_loss           | 40.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 300          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044677714 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.00134     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    value_loss           | 29.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 298          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015258435 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.00245     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    value_loss           | 20.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 297         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007961585 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.00136    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.619       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 295        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01049033 |\n",
      "|    clip_fraction        | 0.0135     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | -0.00169   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.758      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00114   |\n",
      "|    value_loss           | 8.77       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 292         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010542937 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.00103    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.536       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000657   |\n",
      "|    value_loss           | 5.92        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 290          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041539213 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -0.00161     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.22         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.000218     |\n",
      "|    value_loss           | 3.62         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007191037 |\n",
      "|    clip_fraction        | 0.00596     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.000244    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.162       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.000942   |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 291         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010963056 |\n",
      "|    clip_fraction        | 0.0223      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -0.000972   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0669      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 292         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012262982 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | -0.00378    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0904      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 0.892       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 292         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011811389 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | -0.00142    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0952      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00138    |\n",
      "|    value_loss           | 0.579       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 292          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037677805 |\n",
      "|    clip_fraction        | 0.00952      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | -0.0024      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    value_loss           | 0.37         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 289          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054556765 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.886       |\n",
      "|    explained_variance   | 0.000456     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0454       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000633    |\n",
      "|    value_loss           | 0.224        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005453015 |\n",
      "|    clip_fraction        | 0.0165      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.846      |\n",
      "|    explained_variance   | -0.0017     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000515   |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 290          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 148          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039191116 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.884       |\n",
      "|    explained_variance   | -0.000786    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00362     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000151    |\n",
      "|    value_loss           | 0.0964       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007563867 |\n",
      "|    clip_fraction        | 0.00542     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | -0.00121    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0236      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 0.0635      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012170443 |\n",
      "|    clip_fraction        | 0.0619      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.9        |\n",
      "|    explained_variance   | -0.000179   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    value_loss           | 0.0422      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004853756 |\n",
      "|    clip_fraction        | 0.00874     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | -0.0047     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00299    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.000352   |\n",
      "|    value_loss           | 0.0282      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008950388 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | -0.00405    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0043      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 0.0185      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fa75e4e880>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "861774b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0fa97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved_Models', 'PPO_Model_Mountaincar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5504169",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)\n",
    "#del model To delete a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ae5818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad4284b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d34f4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# Solved if avg Reward is 200 or higher, lower the standard deviation the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d8ec210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jainv\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f296a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83854574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing our model(agent) in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75044e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2], dtype=int64), None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9110845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, _ = model.predict(obs)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abbca8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4.2180881e-01,  2.5017478e-04]], dtype=float32),\n",
       " array([-1.], dtype=float32),\n",
       " array([False]),\n",
       " [{}])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b206398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: [-200.]\n",
      "Episode: 2 Score: [-200.]\n",
      "Episode: 3 Score: [-200.]\n",
      "Episode: 4 Score: [-200.]\n",
      "Episode: 5 Score: [-200.]\n"
     ]
    }
   ],
   "source": [
    "episode = 5\n",
    "for episode in range(1,episode+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action,_ = model.predict(obs) #Now using our model to take decision\n",
    "        obs, reward, done, info = env.step([0])\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {}'.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08f6f68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO6'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_6')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "546a4450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a469c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21b0c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved_Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "108e9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 200, verbose = 1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best = stop_callback,\n",
    "                             eval_freq = 5000,\n",
    "                             best_model_save_path = save_path,\n",
    "                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e8dd8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8728a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    fps             | 498      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 374          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020716218 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.000241    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000482    |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010677014 |\n",
      "|    clip_fraction        | 0.0274      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 88.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 337          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064967293 |\n",
      "|    clip_fraction        | 0.00601      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.0018      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 83.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011444884 |\n",
      "|    clip_fraction        | 0.0481      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.00625     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00526    |\n",
      "|    value_loss           | 69.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    fps             | 318      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -200          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 316           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045652178 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.07         |\n",
      "|    explained_variance   | 0.00313       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.18          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 0.00014       |\n",
      "|    value_loss           | 54.3          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 312         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012645612 |\n",
      "|    clip_fraction        | 0.024       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.000667   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.14        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00321    |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4912\\2179392462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    305\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         return super().learn(\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;31m# Clip grad norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a9dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
